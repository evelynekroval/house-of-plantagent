{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47996c2",
   "metadata": {},
   "source": [
    "# Nutrition Database\n",
    "\n",
    "## Goal\n",
    "\n",
    "- [ ] Research with Copilot / ChatGPT how to attain this...\n",
    "    - [ ] Do I want to scrape?\n",
    "    - [ ] Do I want to API?\n",
    "- [ ] Test it out on the below output from recipe API..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c40b496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Energy Bars, Banana/maple/walnut [Vegan]',\n",
       "  'ingredients': ['2 Bananas; over-ripe',\n",
       "   '3 tb Maple syrup',\n",
       "   '3 tb Walnuts; toasted',\n",
       "   '2 tb Flour; vital gluten',\n",
       "   '1 1/3 c Rolled oats',\n",
       "   '1/2 c Oat bran',\n",
       "   '1 ts Vanilla extract',\n",
       "   '2 ts Lecithin; granules',\n",
       "   '1/4 ts Salt'],\n",
       "  'servings': '6 Servings',\n",
       "  'instructions': 'Combine all ingredients in cusinart and blend until a nearly smooth paste. Spread in 8\"x8\" or 9\"x9\" pan or drop on cookie sheet and shape into \"cliff bar\" style bars. Bake at 350 for 15 minutes. Let cool completely before eating. NOTE: these are not as \"shiny\" as the other style energy bars. 210 calories per 1/6th recipe/per bar fat: 5.9g sat. fat: .8g cholesterol: 0 carbohydrate: 34.9g dietary fiber:4.4g protein: 9.5g sodium: 9.mg'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recipe output from Ninja API\n",
    "[{'title': 'Energy Bars, Banana/maple/walnut [Vegan]', 'ingredients': ['2 Bananas; over-ripe', '3 tb Maple syrup', '3 tb Walnuts; toasted', '2 tb Flour; vital gluten', '1 1/3 c Rolled oats', '1/2 c Oat bran', '1 ts Vanilla extract', '2 ts Lecithin; granules', '1/4 ts Salt'], 'servings': '6 Servings', 'instructions': 'Combine all ingredients in cusinart and blend until a nearly smooth paste. Spread in 8\"x8\" or 9\"x9\" pan or drop on cookie sheet and shape into \"cliff bar\" style bars. Bake at 350 for 15 minutes. Let cool completely before eating. NOTE: these are not as \"shiny\" as the other style energy bars. 210 calories per 1/6th recipe/per bar fat: 5.9g sat. fat: .8g cholesterol: 0 carbohydrate: 34.9g dietary fiber:4.4g protein: 9.5g sodium: 9.mg'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381fa65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Since it's a JSON file, do I need to transform it somehow in order to process it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b4cfd6",
   "metadata": {},
   "source": [
    "# Nutrition Database — Actionable To‑Do List\n",
    "\n",
    "What I did\n",
    "- Converted the previous guidance into a focused, action‑oriented to‑do list that only covers the nutritional database approach. This file is structured so you can pick tasks top-to-bottom and make measurable progress.\n",
    "\n",
    "How to use this list\n",
    "- Work in order unless you have a specific need (e.g., you already have a preferred DB).\n",
    "- Each item includes a short why, concrete steps, estimated effort, and acceptance criteria.\n",
    "- Mark checkboxes as you complete items.\n",
    "\n",
    "Priority legend\n",
    "- P0 = immediate / unblocker\n",
    "- P1 = essential next steps\n",
    "- P2 = useful improvements\n",
    "- P3 = optional / future\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P0 — Decide core approach & tooling (critical first decision)\n",
    "- [ ] Choose data sources (pick at least one baseline + one product source)\n",
    "  - Options: USDA FoodData Central (baseline, public domain), Open Food Facts (barcoded products, ODbL), Commercial API (Nutritionix/Edamam/Spoonacular) for branded coverage.\n",
    "  - Effort: 10–20m\n",
    "  - Acceptance: decision recorded in README or this file.\n",
    "\n",
    "- [ ] Choose a local storage backend\n",
    "  - Recommended for notebooks: DuckDB (zero‑admin, fast for bulk analytics). Alternative: SQLite (simple) or PostgreSQL (production).\n",
    "  - Effort: 10–20m\n",
    "  - Acceptance: backend choice noted and one empty DB file created (e.g., data/nutrition.duckdb).\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P0 — Acquire official bulk datasets (legal & reproducible)\n",
    "- [ ] Download USDA FoodData Central bulk files (JSON/CSV)\n",
    "  - Steps:\n",
    "    - Fetch latest release from https://fdc.nal.usda.gov/ or FDC bulk download page.\n",
    "    - Save source file(s) under data/raw/usda/ with a snapshot date and filename.\n",
    "  - Effort: 15–30m\n",
    "  - Acceptance: file(s) present in data/raw/usda/ and a metadata note (URL + date).\n",
    "\n",
    "- [ ] Download Open Food Facts product dump (optional but highly recommended)\n",
    "  - Steps:\n",
    "    - Fetch product dump (country/global) from Open Food Facts dumps.\n",
    "    - Save under data/raw/off/ and record license (ODbL).\n",
    "  - Effort: 15–30m\n",
    "  - Acceptance: file(s) present in data/raw/off/ with metadata & license note.\n",
    "\n",
    "- [ ] (Optional) Evaluate a commercial API for branded coverage\n",
    "  - Steps:\n",
    "    - Create free/trial accounts with Nutritionix, Edamam, or Spoonacular; check request limits and pricing.\n",
    "    - Document API capabilities and cost.\n",
    "  - Effort: 30–60m\n",
    "  - Acceptance: a short comparison note in data_catalog/README.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P1 — Ingestion & canonical schema\n",
    "- [ ] Define a minimal canonical schema for food items\n",
    "  - Suggested fields:\n",
    "    - source (usda | off | api:name)\n",
    "    - source_id (fdc_id | barcode | api_id)\n",
    "    - name (primary description)\n",
    "    - normalized_name (lowercased, stripped)\n",
    "    - category (e.g., \"Fruits\", \"Dairy\")\n",
    "    - serving_sizes (list of {label, grams})\n",
    "    - nutrients (structured JSON or normalized nutrient columns; include kcal, protein, fat, carbs, fiber, sugar, sodium at minimum)\n",
    "    - last_updated (snapshot date)\n",
    "  - Effort: 30–60m\n",
    "  - Acceptance: schema document saved at data_catalog/schema.md\n",
    "\n",
    "- [ ] Implement ingestion script for USDA → local DB\n",
    "  - Steps:\n",
    "    - Read USDA JSON/CSV.\n",
    "    - Extract fields into canonical schema.\n",
    "    - Normalize nutrient basis (convert per-serving to per-100g if necessary).\n",
    "    - Write into DuckDB table `foods_usda` (or chosen backend).\n",
    "  - Effort: 2–4 hours\n",
    "  - Acceptance: DB contains `foods_usda` with sample records and correct nutrient columns.\n",
    "\n",
    "- [ ] Implement ingestion script for Open Food Facts → local DB\n",
    "  - Steps:\n",
    "    - Read OFF JSON/CSV.\n",
    "    - Map nutriments and barcodes to canonical schema.\n",
    "    - Write into DuckDB table `foods_off`.\n",
    "    - Record OFF license/attribution metadata in DB or data_catalog.\n",
    "  - Effort: 2–4 hours\n",
    "  - Acceptance: DB contains `foods_off` with sample records and license noted.\n",
    "\n",
    "- [ ] Add ingestion metadata tracking\n",
    "  - Steps:\n",
    "    - Store dataset filename, source URL, download date, file SHA/size in a `data_snapshots` table or YAML metadata.\n",
    "  - Effort: 30–60m\n",
    "  - Acceptance: `data_snapshots` populated after each import.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P1 — Build core lookup & matching pipeline\n",
    "- [ ] Create normalized-name utilities (normalization rules)\n",
    "  - Steps:\n",
    "    - Lowercase, strip punctuation, remove parenthetical text, collapse multi-spaces.\n",
    "    - Maintain synonyms map (e.g., \"scallions\" → \"green onion\").\n",
    "  - Effort: 1–2 hours\n",
    "  - Acceptance: normalized_name(\"1 cup chopped Onion (peeled)\") → \"onion\"\n",
    "\n",
    "- [ ] Implement full-text search index for names\n",
    "  - Steps:\n",
    "    - Use DuckDB FTS or SQLite FTS (or create simple token index with trigram).\n",
    "    - Populate index with normalized names and common synonyms.\n",
    "  - Effort: 1–3 hours\n",
    "  - Acceptance: fast keyword search returns candidates.\n",
    "\n",
    "- [ ] Implement fuzzy-matching fallback using rapidfuzz\n",
    "  - Steps:\n",
    "    - Query FTS for candidates, score with rapidfuzz, return top N.\n",
    "    - Define confidence thresholds (e.g., >90% exact, 70–90% candidate, <70% low).\n",
    "  - Effort: 1–2 hours\n",
    "  - Acceptance: function find_candidates(\"brown sugar\") returns sensible top-3 matches with scores.\n",
    "\n",
    "- [ ] Add match-selection & provenance\n",
    "  - Steps:\n",
    "    - For each match, return: source, source_id, name, normalized_name, score.\n",
    "    - If ambiguous, surface top N or require human confirmation in notebook/CLI.\n",
    "  - Effort: 1–2 hours\n",
    "  - Acceptance: outputs include provenance and confidence.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P1 — Unit conversion, density table & ingredient parsing (minimal)\n",
    "- [ ] Create a small density/serving lookup table\n",
    "  - Steps:\n",
    "    - CSV or JSON mapping like {item: {cup_g: X, tbsp_g: Y, piece_g: Z, density_g_per_ml: Z}} for top ~40 common items (flour, sugar, oil, rice, butter, milk, vegetables, fruits).\n",
    "    - Source values from USDA, published cookbooks, or measured standards; record sources.\n",
    "  - Effort: 2–4 hours\n",
    "  - Acceptance: density table present at data/densities.csv and a README entry describing sources.\n",
    "\n",
    "- [ ] Implement unit conversion util: to_grams(qty, unit, normalized_name)\n",
    "  - Steps:\n",
    "    - Use pint or a small custom mapping to convert units to grams using densities and per-servings.\n",
    "    - Fallback: if no density available, warn and use category-average or request user selection.\n",
    "  - Effort: 2–4 hours\n",
    "  - Acceptance: to_grams(\"1\", \"cup\", \"all-purpose flour\") ≈ 120g (testable sample).\n",
    "\n",
    "- [ ] Minimal ingredient parsing helper\n",
    "  - Steps:\n",
    "    - Lightweight parser (regex + heuristics) that extracts quantity, unit, and item text from a recipe line.\n",
    "    - Produce output suitable to feed to to_grams() and matching pipeline.\n",
    "  - Effort: 2–4 hours\n",
    "  - Acceptance: parse_ingredient(\"1 1/2 cups chopped carrots\") → {qty:1.5, unit:\"cup\", name:\"carrots\"}\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P1 — Nutrient scaling, summation & confidence\n",
    "- [ ] Implement nutrient scaling function\n",
    "  - Steps:\n",
    "    - Standard formula: nutrient_total = (grams / reference_grams) * nutrient_value (make sure nutrient units match, e.g., mg/g).\n",
    "    - Normalize nutrient basis in DB entries (prefer per 100 g).\n",
    "  - Effort: 1–2 hours\n",
    "  - Acceptance: Unit tests show correct scaling for sample food entry.\n",
    "\n",
    "- [ ] Sum recipe-level nutrients and attach provenance & confidence per ingredient\n",
    "  - Steps:\n",
    "    - For each ingredient: grams, matched source, score, per-ingredient nutrients.\n",
    "    - Sum nutrients across ingredients; produce a result payload with breakdown and flags for low-confidence matches.\n",
    "  - Effort: 1–2 hours\n",
    "  - Acceptance: example run for a small recipe returns totals + per-ingredient provenance.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P2 — Caching & hybrid strategy for branded items\n",
    "- [ ] Implement local caching layer for API lookups\n",
    "  - Steps:\n",
    "    - If using a commercial API for branded products, cache responses in local DB to reduce cost and latency.\n",
    "  - Effort: 1–2 hours\n",
    "  - Acceptance: cache hit reduces API calls; cache table contains API responses + timestamp.\n",
    "\n",
    "- [ ] Decision rule for hybrid lookup\n",
    "  - Steps:\n",
    "    - Pipeline: attempt exact DB match → FTS/fuzzy → if low confidence and API available, fall back to API → cache result.\n",
    "  - Effort: 1 hour\n",
    "  - Acceptance: documented in pipeline README.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P2 — Provenance, licensing & governance\n",
    "- [ ] Create data_catalog/README with:\n",
    "  - List of sources and exact download URLs\n",
    "  - Snapshot dates and file checksums\n",
    "  - License obligations (USDA public domain, OFF ODbL share‑alike, commercial API terms)\n",
    "  - Effort: 1–2 hours\n",
    "  - Acceptance: data_catalog/README added to repo.\n",
    "\n",
    "- [ ] Ensure each DB record retains source and source_id field\n",
    "  - Effort: 30–60m\n",
    "  - Acceptance: every row has source and source_id filled.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P2 — Tests & reproducibility\n",
    "- [ ] Add unit tests for:\n",
    "  - Ingestion (sample fixture)\n",
    "  - Normalization / matching\n",
    "  - to_grams conversion\n",
    "  - Nutrient scaling\n",
    "  - Effort: 3–6 hours\n",
    "  - Acceptance: tests run locally and cover core cases.\n",
    "\n",
    "- [ ] Create a small example notebook showing ingestion → match → calculation\n",
    "  - Effort: 2–4 hours\n",
    "  - Acceptance: notebook executes end-to-end on a fresh environment using the local DB.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "P3 — Enhancements (future)\n",
    "- [ ] Add retention factors / cooked vs raw conversions (USDA retention factors)\n",
    "- [ ] Expand density table coverage (crowdsource corrections)\n",
    "- [ ] Add embedding/semantic matching for hard cases\n",
    "- [ ] Add CLI or tiny API for external usage\n",
    "- [ ] Periodic automated dataset refresh (cron / GitHub Actions) with snapshot audits\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Suggested immediate next actions (pick one)\n",
    "- A: Create data/raw/ and download USDA + OFF snapshots and record metadata (P0+P1 ingest prep)\n",
    "- B: Scaffold ingestion script for USDA into DuckDB with canonical schema (P1 ingestion)\n",
    "- C: Create density CSV and implement to_grams + a few unit tests (P1 conversions)\n",
    "\n",
    "Tell me which immediate action you want and I will generate the first script or file for that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1eaf01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house-of-plantagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
